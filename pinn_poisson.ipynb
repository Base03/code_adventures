{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    _ = torch.tensor([1.0], device=device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, num_hidden_layers=2, hidden_dim=32):\n",
    "        super(PINN, self).__init__()\n",
    "        self.input = nn.Linear(3, hidden_dim, bias=False)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim, bias=False) for _ in range(num_hidden_layers)])\n",
    "        self.output = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        coords = torch.stack((x, y, z), dim=1).to(x.device)\n",
    "        X = self.input(coords)\n",
    "        X = self.activation(X)\n",
    "        for layer in self.hidden_layers:\n",
    "            X = layer(X)\n",
    "            X = self.activation(X)\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.input.weight)\n",
    "        #nn.init.zeros_(self.input.bias)\n",
    "        for layer in self.hidden_layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            #nn.init.zeros_(layer.bias)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        #nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def compute_derivatives(self, x, y, z):\n",
    "        x = x.requires_grad_(True)\n",
    "        y = y.requires_grad_(True)\n",
    "        z = z.requires_grad_(True)\n",
    "        u = self.forward(x, y, z)\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_z = torch.autograd.grad(u, z, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "        u_zz = torch.autograd.grad(u_z, z, grad_outputs=torch.ones_like(u_z), create_graph=True)[0]\n",
    "        return u, u_x, u_y, u_z, u_xx, u_yy, u_zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define source and boundary functions\n",
    "# lets make something simple\n",
    "# u = x^2 + y^2 + z^2\n",
    "# u_xx + u_yy + u_zz = 6\n",
    "# therefore, g = 6 s.t. u_xx + u_yy + u_zz = g\n",
    "# simplest B.C. is therefore u =  x^2 + y^2 + z^2\n",
    "def g(x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "    return 3 * torch.ones_like(x) \n",
    "\n",
    "def h(x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "    return x**2 + y**2 + z**2\n",
    "    # return torch.zeros_like(x) # u = 0 on boundary\n",
    "\n",
    "def f(x: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "    return x**2\n",
    "    #return x**2 + y**2 + z**2\n",
    "    # return torch.zeros_like(x) # ignore the exact solution for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training data\n",
    "def generate_training_points(n_interior: int = 1024, n_boundary: int = 1024) -> Tuple[torch.Tensor, ...]:\n",
    "    # interior points\n",
    "    coords_i = torch.rand(n_interior, 3) * 2 - 1\n",
    "    coords_i = coords_i.to(device)\n",
    "\n",
    "    # boundary points\n",
    "    coords_b = torch.rand(n_boundary, 3) * 2 - 1\n",
    "    dim_set = torch.randint(0, 3, (n_boundary,)) # (B,)\n",
    "    val_set = torch.randint(0, 2, (n_boundary,)).float() * 2 - 1 # (B,)\n",
    "    coords_b[torch.arange(n_boundary), dim_set] = val_set\n",
    "    coords_b = coords_b.to(device)\n",
    "\n",
    "    return coords_i, coords_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords_i, coords_b = generate_training_points(n_interior=16, n_boundary=16)\n",
    "# print('Interior points:', coords_i.data)\n",
    "# print('Boundary points:', coords_b.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pt/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 0.266347\n",
      "gradient norm: 0.3483726680278778\n",
      "input.weight grad norm: 0.3483726680278778\n",
      "hidden_layers.0.weight grad norm: 0.4650894105434418\n",
      "hidden_layers.1.weight grad norm: 0.4969950318336487\n",
      "output.weight grad norm: 0.34685638546943665\n",
      "\n",
      "Epoch 1000, Training Loss: 0.202031\n",
      "gradient norm: 0.00028335335082374513\n",
      "input.weight grad norm: 0.00028335335082374513\n",
      "hidden_layers.0.weight grad norm: 0.00029684562468901277\n",
      "hidden_layers.1.weight grad norm: 0.0002264185022795573\n",
      "output.weight grad norm: 0.0001639864785829559\n",
      "\n",
      "Epoch 2000, Training Loss: 0.201577\n",
      "gradient norm: 0.00031365646282210946\n",
      "input.weight grad norm: 0.00031365646282210946\n",
      "hidden_layers.0.weight grad norm: 0.00028123738593421876\n",
      "hidden_layers.1.weight grad norm: 0.00018164599896408617\n",
      "output.weight grad norm: 0.00015513415564782917\n",
      "\n",
      "Epoch 3000, Training Loss: 0.201085\n",
      "gradient norm: 0.00014419657236430794\n",
      "input.weight grad norm: 0.00014419657236430794\n",
      "hidden_layers.0.weight grad norm: 0.00018351897597312927\n",
      "hidden_layers.1.weight grad norm: 0.00015609261754434556\n",
      "output.weight grad norm: 0.00012458219134714454\n",
      "\n",
      "Epoch 4000, Training Loss: 0.199869\n",
      "gradient norm: 0.00027351538301445544\n",
      "input.weight grad norm: 0.00027351538301445544\n",
      "hidden_layers.0.weight grad norm: 0.00032697198912501335\n",
      "hidden_layers.1.weight grad norm: 0.0005557683180086315\n",
      "output.weight grad norm: 0.0002328913105884567\n",
      "\n",
      "Epoch 5000, Training Loss: 0.198012\n",
      "gradient norm: 0.0001531871675979346\n",
      "input.weight grad norm: 0.0001531871675979346\n",
      "hidden_layers.0.weight grad norm: 0.0002976439136546105\n",
      "hidden_layers.1.weight grad norm: 0.00047090958105400205\n",
      "output.weight grad norm: 0.0003263498074375093\n",
      "\n",
      "Epoch 6000, Training Loss: 0.196381\n",
      "gradient norm: 0.0001921742659760639\n",
      "input.weight grad norm: 0.0001921742659760639\n",
      "hidden_layers.0.weight grad norm: 0.0002735982707235962\n",
      "hidden_layers.1.weight grad norm: 0.000472317507956177\n",
      "output.weight grad norm: 0.000348381872754544\n",
      "\n",
      "Epoch 7000, Training Loss: 0.194734\n",
      "gradient norm: 0.0002905366418417543\n",
      "input.weight grad norm: 0.0002905366418417543\n",
      "hidden_layers.0.weight grad norm: 0.0009246363188140094\n",
      "hidden_layers.1.weight grad norm: 0.001367627759464085\n",
      "output.weight grad norm: 0.0010371742537245154\n",
      "\n",
      "Epoch 8000, Training Loss: 0.193185\n",
      "gradient norm: 0.0016506245592609048\n",
      "input.weight grad norm: 0.0016506245592609048\n",
      "hidden_layers.0.weight grad norm: 0.005897023715078831\n",
      "hidden_layers.1.weight grad norm: 0.013946535997092724\n",
      "output.weight grad norm: 0.009337659925222397\n",
      "\n",
      "Epoch 9000, Training Loss: 0.191945\n",
      "gradient norm: 0.0006376685341820121\n",
      "input.weight grad norm: 0.0006376685341820121\n",
      "hidden_layers.0.weight grad norm: 0.0023947604931890965\n",
      "hidden_layers.1.weight grad norm: 0.005516288336366415\n",
      "output.weight grad norm: 0.0035435284953564405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "output_every = 1000\n",
    "\n",
    "n_interior = 4096\n",
    "n_boundary = 4096\n",
    "\n",
    "lr = 1e-3\n",
    "lambda_b = 1.0\n",
    "lambda_i = 1.0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "model = PINN(num_hidden_layers=2, hidden_dim=16).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100)\n",
    "# use a cosine scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "# training data\n",
    "coords_i, coords_b = generate_training_points(n_interior, n_boundary)\n",
    "coords_i = coords_i.to(device)\n",
    "coords_b = coords_b.to(device)\n",
    "\n",
    "x_i, y_i, z_i = coords_i[:, 0], coords_i[:, 1], coords_i[:, 2]\n",
    "x_b, y_b, z_b = coords_b[:, 0], coords_b[:, 1], coords_b[:, 2]\n",
    "\n",
    "# # validation data\n",
    "# coords_i, coords_b = generate_training_points(n_interior, n_boundary)\n",
    "# coords_i = coords_i.to(device)\n",
    "# coords_b = coords_b.to(device)\n",
    "\n",
    "# x_v_i, y_v_i, z_v_i = coords_i[:, 0], coords_i[:, 1], coords_i[:, 2]\n",
    "# x_v_b, y_v_b, z_v_b = coords_b[:, 0], coords_b[:, 1], coords_b[:, 2]\n",
    "\n",
    "\n",
    "# print('Interior points:')\n",
    "# print('x_i:', x_i.data)\n",
    "# print('y_i:', y_i.data)\n",
    "# print('z_i:', z_i.data)\n",
    "# print('Boundary points:')\n",
    "# print('x_b:', x_b.data)\n",
    "# print('y_b:', y_b.data)\n",
    "# print('z_b:', z_b.data)\n",
    "\n",
    "# # check everything is on the right device\n",
    "# print('x_i device:', x_i.device)\n",
    "# print('y_i device:', y_i.device)\n",
    "# print('z_i device:', z_i.device)\n",
    "# print('x_b device:', x_b.device)\n",
    "# print('y_b device:', y_b.device)\n",
    "# print('z_b device:', z_b.device)\n",
    "# # check model is on the right device\n",
    "# print('Model device:', next(model.parameters()).device)\n",
    "# # check model is on the right device\n",
    "# print('Model device:', model.input.weight.device)\n",
    "# # check model is on the right device\n",
    "# print('Model device:', model.hidden_layers[0].weight.device)\n",
    "# # check model is on the right device\n",
    "# print('Model device:', model.output.weight.device)\n",
    "# # check model is on the right device\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # u_predict, u_x, u_y, u_z, u_xx, u_yy, u_zz = model.compute_derivatives(x_i, y_i, z_i)\n",
    "    # u_predict = model(x_i, y_i, z_i)\n",
    "\n",
    "    # # interior points:\n",
    "    # g_i = g(x_i, y_i, z_i)\n",
    "    # laplacian = u_xx + u_yy + u_zz\n",
    "    # loss_i = torch.mean((laplacian - g_i)**2) * lambda_i\n",
    "\n",
    "    # # boundary points:\n",
    "    # u_predict_b = model(x_b, y_b, z_b)\n",
    "    # h_b = h(x_b, y_b, z_b)\n",
    "    # loss_b = torch.sqrt(torch.mean((u_predict_b - h_b)**2)) * lambda_b\n",
    "\n",
    "    # total loss\n",
    "    # loss = loss_i + loss_b\n",
    "    # loss = loss_b\n",
    "    loss = torch.mean((model(x_i, y_i, z_i) - f(x_i, y_i, z_i))**2) * lambda_i\n",
    "\n",
    "    # optimization step\n",
    "    loss.backward()\n",
    "    # clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    if epoch % output_every == 0:\n",
    "        print(f\"Epoch {epoch}, Training Loss: {loss.item():.6f}\")\n",
    "        print(f\"gradient norm: {model.input.weight.grad.norm()}\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name} grad norm: {param.grad.norm().item()}\")\n",
    "        \n",
    "        print()\n",
    "        # print(f\"Epoch {epoch}, Training Loss: {loss.item():.6f}, \"\n",
    "        #           f\"PDE Loss: {loss_i.item():.6f}, \"\n",
    "        #           f\"BC Loss: {loss_b.item():.6f}\")\n",
    "        \n",
    "        # # validation\n",
    "        # with torch.no_grad():\n",
    "        #     u_predict = model(x_v_i, y_v_i, z_v_i)\n",
    "        #     u_exact = f(x_v_i, y_v_i, z_v_i)\n",
    "        #     loss_v_i = torch.mean((u_predict - u_exact)**2) * lambda_i\n",
    "\n",
    "        #     h_predict_b = model(x_v_b, y_v_b, z_v_b)\n",
    "        #     h_exact_b = h(x_v_b, y_v_b, z_v_b)\n",
    "        #     loss_v_b = torch.mean((h_predict_b - h_exact_b)**2) * lambda_b\n",
    "\n",
    "        #     total_loss_v = loss_v_i + loss_v_b\n",
    "        #     print(f\"(u_model - u)**2: {loss_v_i.item():.6f}, \"\n",
    "        #           f\"(h_model - h)**2: {loss_v_b.item():.6f}, \"\n",
    "        #           f\"Total Validation Loss: {total_loss_v.item():.6f}\")\n",
    "\n",
    "        # print(f\"Epoch {epoch}, Validation Loss: {loss.item():.6f}, \"\n",
    "        #           f\"PDE Loss: {loss_i.item():.6f}, \"\n",
    "        #           f\"BC Loss: {loss_b.item():.6f}\")\n",
    "        # print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: [[-0.05503023]\n",
      " [ 0.        ]\n",
      " [ 0.05503023]]\n",
      " f: [1. 0. 1.] boundary [1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "x = [1,0,-1]\n",
    "y = [0,0,0]\n",
    "z = [0,0,0]\n",
    "x = torch.tensor(x).float().to(device)\n",
    "y = torch.tensor(y).float().to(device)\n",
    "z = torch.tensor(z).float().to(device)\n",
    "u = model(x, y, z).cpu().detach().numpy()\n",
    "exact = f(x, y, z).cpu().detach().numpy()\n",
    "boundary = h(x, y, z).cpu().detach().numpy()\n",
    "\n",
    "print(f\"u: {u}\\n f: {exact} boundary {boundary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a look at the weights and bias values after training. what histogram should we expect? For a well trained model, we should expect the weights to be normally distributed and the biases to be uniformly distributed.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_weights(model: nn.Module):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i, layer in enumerate(model.hidden_layers):\n",
    "        weights.append(layer.weight.data.cpu().numpy())\n",
    "        biases.append(layer.bias.data.cpu().numpy())\n",
    "        print(f\"layer {i}:\")\n",
    "        print(f\"weights: min: {weights[i].min()}, max: {weights[i].max()}, mean: {weights[i].mean()}, std: {weights[i].std()}\")\n",
    "        print(f\"biases: min: {biases[i].min()}, max: {biases[i].max()}, mean: {biases[i].mean()}, std: {biases[i].std()}\")\n",
    "    \n",
    "    \n",
    "plot_weights(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
