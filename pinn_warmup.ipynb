{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim:int=3, output_dim:int=1, num_hidden_layers:int=2, hidden_dim:int=32):\n",
    "        \"\"\"\n",
    "        Initialize the Physics-Informed Neural Network (PINN).\n",
    "        input_dim: Dimension of the input (e.g., x, y, z).\n",
    "        output_dim: Dimension of the output (e.g., u).\n",
    "        num_hidden_layers: Number of hidden layers in the network.\n",
    "        hidden_dim: Number of neurons in each hidden layer.\n",
    "        \"\"\"\n",
    "        super(PINN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "\n",
    "        self.input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers)])\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        X: (B, D) tensor, where B is the batch size and D is the input dimension.\n",
    "        Returns: (B, output_dim) tensor\n",
    "        \"\"\"\n",
    "        assert X.shape[1] == self.input_dim, f\"Input dimension should be {self.input_dim}, but got {X.shape[1]}\"\n",
    "\n",
    "        X = self.input(X)\n",
    "        X = self.activation(X)\n",
    "        for layer in self.hidden_layers:\n",
    "            #X = self.activation(layer(X)) + X\n",
    "            X = torch.sin(layer(X)) + X\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the network using Xavier uniform distribution. (tanh activation)\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.input.weight)\n",
    "        nn.init.zeros_(self.input.bias)\n",
    "        for layer in self.hidden_layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def compute_derivatives(self, X: torch.Tensor):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple functions for testing ability to fit\n",
    "def sum_square(X: torch.Tensor) -> torch.Tensor:\n",
    "    Y = X*X\n",
    "    return torch.sum(Y, dim=1).unsqueeze(1)\n",
    "\n",
    "def prod_sin(X: torch.Tensor, k:float = torch.pi) -> torch.Tensor:\n",
    "    Y = torch.sin(X*k)\n",
    "    return torch.prod(Y, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_points(n_interior: int = 1024, n_boundary: int = 1024, dim:int = 3) -> Tuple[torch.Tensor, ...]:\n",
    "    \"\"\" \n",
    "    Generate training points on [-1,1]^dim\n",
    "    n_interior: Number of interior points.\n",
    "    n_boundary: Number of boundary points.\n",
    "    dim: Dimension of the input space.\n",
    "    Returns: Tuple of interior and boundary points each of shape (n_points, dim)\n",
    "    \"\"\"\n",
    "    # interior points\n",
    "    coords_i = torch.rand(n_interior, dim) * 2 - 1\n",
    "    coords_i = coords_i.to(device)\n",
    "\n",
    "    # boundary points\n",
    "    coords_b = torch.rand(n_boundary, dim) * 2 - 1\n",
    "    dim_set = torch.randint(0, dim, (n_boundary,)) # (B,)\n",
    "    val_set = torch.randint(0, 2, (n_boundary,)).float() * 2 - 1 # (B,)\n",
    "    coords_b[torch.arange(n_boundary), dim_set] = val_set\n",
    "    coords_b = coords_b.to(device)\n",
    "\n",
    "    return coords_i, coords_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interior points shape: torch.Size([4096, 3])\n",
      "Interior points target shape: torch.Size([4096, 1])\n",
      "Boundary points shape: torch.Size([4096, 3])\n",
      "Boundary points target shape: torch.Size([4096, 1])\n",
      "Epoch 0, Training Loss: 4.882177\n",
      "Epoch 1000, Training Loss: 0.067167\n",
      "Epoch 2000, Training Loss: 0.060663\n",
      "Epoch 3000, Training Loss: 0.038946\n",
      "Epoch 4000, Training Loss: 0.026490\n",
      "Epoch 5000, Training Loss: 0.025903\n",
      "Epoch 6000, Training Loss: 0.012183\n",
      "Epoch 7000, Training Loss: 0.011531\n",
      "Epoch 8000, Training Loss: 0.008278\n",
      "Epoch 9000, Training Loss: 0.006879\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "output_every = 1000\n",
    "\n",
    "n_interior = 4096\n",
    "n_boundary = 4096\n",
    "dim = 3\n",
    "\n",
    "lr = 1e-3\n",
    "w_i = 1.0\n",
    "w_b = 1.0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "model = PINN(input_dim=dim, output_dim=1, num_hidden_layers=8, hidden_dim=32).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# training data\n",
    "coords_i, coords_b = generate_training_points(n_interior=n_interior, n_boundary=n_boundary, dim=dim)\n",
    "coords_i = coords_i.to(device)\n",
    "coords_b = coords_b.to(device)\n",
    "\n",
    "# y_i = sum_square(coords_i).to(device)\n",
    "# y_b = sum_square(coords_b).to(device)\n",
    "\n",
    "y_i = prod_sin(coords_i, torch.pi).to(device)\n",
    "y_b = prod_sin(coords_b, torch.pi).to(device)\n",
    "\n",
    "print(f\"Interior points shape: {coords_i.shape}\")\n",
    "print(f\"Interior points target shape: {y_i.shape}\")\n",
    "print(f\"Boundary points shape: {coords_b.shape}\")\n",
    "print(f\"Boundary points target shape: {y_b.shape}\")\n",
    "\n",
    "# print(f\"Interior points: {coords_i.cpu().detach().numpy()}\")\n",
    "# print(f\"Interior points target: {y_i.cpu().detach().numpy()}\")\n",
    "# print(f\"Boundary points: {coords_b.cpu().detach().numpy()}\")\n",
    "# print(f\"Boundary points target: {y_b.cpu().detach().numpy()}\")\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # predictions\n",
    "    out_i = model(coords_i)\n",
    "    out_b = model(coords_b)\n",
    "    # loss\n",
    "    loss_i = criterion(out_i, y_i)\n",
    "    loss_b = criterion(out_b, y_b)\n",
    "    loss = w_i * torch.sqrt(loss_i) + w_b * torch.sqrt(loss_b)\n",
    "\n",
    "    # print(f\"out_i: {out_i.shape}, y_i: {y_i.shape}\")\n",
    "    # print(f\"out_b: {out_b.shape}, y_b: {y_b.shape}\")\n",
    "    # print(f\"loss_i: {loss_i.shape}, loss_b: {loss_b.shape}\")\n",
    "    # print(f\"loss: {loss.shape}\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch % output_every == 0:\n",
    "        print(f\"Epoch {epoch}, Training Loss: {loss.item():.6f}\")\n",
    "        # print(f\"gradient norm: {model.input.weight.grad.norm()}\")\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(f\"{name} grad norm: {param.grad.norm().item()}\")\n",
    "        \n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS Iteration 0, Training Loss: 0.006482\n",
      "LBFGS Iteration 10, Training Loss: 0.004522\n",
      "LBFGS Iteration 20, Training Loss: 0.003680\n",
      "LBFGS Iteration 30, Training Loss: 0.003051\n",
      "LBFGS Iteration 40, Training Loss: 0.002611\n",
      "LBFGS Iteration 50, Training Loss: 0.002240\n",
      "LBFGS Iteration 60, Training Loss: 0.002033\n",
      "LBFGS Iteration 70, Training Loss: 0.001882\n",
      "LBFGS Iteration 80, Training Loss: 0.001735\n",
      "LBFGS Iteration 90, Training Loss: 0.001589\n",
      "LBFGS Iteration 100, Training Loss: 0.001466\n",
      "LBFGS Iteration 110, Training Loss: 0.001381\n",
      "LBFGS Iteration 120, Training Loss: 0.001299\n",
      "LBFGS Iteration 130, Training Loss: 0.001217\n",
      "LBFGS Iteration 140, Training Loss: 0.001159\n",
      "LBFGS Iteration 150, Training Loss: 0.001116\n",
      "LBFGS Iteration 160, Training Loss: 0.001076\n",
      "LBFGS Iteration 170, Training Loss: 0.001039\n",
      "LBFGS Iteration 180, Training Loss: 0.001005\n",
      "LBFGS Iteration 190, Training Loss: 0.000980\n",
      "LBFGS Iteration 200, Training Loss: 0.000951\n",
      "LBFGS Iteration 210, Training Loss: 0.000926\n",
      "LBFGS Iteration 220, Training Loss: 0.000892\n",
      "LBFGS Iteration 230, Training Loss: 0.000864\n",
      "LBFGS Iteration 240, Training Loss: 0.000832\n",
      "LBFGS Iteration 250, Training Loss: 0.000808\n",
      "LBFGS Iteration 260, Training Loss: 0.000785\n",
      "LBFGS Iteration 270, Training Loss: 0.000760\n",
      "LBFGS Iteration 280, Training Loss: 0.000736\n",
      "LBFGS Iteration 290, Training Loss: 0.000715\n",
      "LBFGS Iteration 300, Training Loss: 0.000696\n",
      "LBFGS Iteration 310, Training Loss: 0.000678\n",
      "LBFGS Iteration 320, Training Loss: 0.000662\n",
      "LBFGS Iteration 330, Training Loss: 0.000647\n",
      "LBFGS Iteration 340, Training Loss: 0.000635\n",
      "LBFGS Iteration 350, Training Loss: 0.000618\n",
      "LBFGS Iteration 360, Training Loss: 0.000601\n",
      "LBFGS Iteration 370, Training Loss: 0.000584\n",
      "LBFGS Iteration 380, Training Loss: 0.000571\n",
      "LBFGS Iteration 390, Training Loss: 0.000560\n",
      "LBFGS Iteration 400, Training Loss: 0.000550\n",
      "LBFGS Iteration 410, Training Loss: 0.000542\n",
      "LBFGS Iteration 420, Training Loss: 0.000537\n",
      "LBFGS Iteration 430, Training Loss: 0.000531\n",
      "LBFGS Iteration 440, Training Loss: 0.000526\n",
      "LBFGS Iteration 450, Training Loss: 0.000520\n",
      "LBFGS Iteration 460, Training Loss: 0.000514\n",
      "LBFGS Iteration 470, Training Loss: 0.000510\n",
      "LBFGS Iteration 480, Training Loss: 0.000507\n",
      "LBFGS Iteration 490, Training Loss: 0.000505\n",
      "LBFGS Iteration 500, Training Loss: 0.000500\n",
      "LBFGS Iteration 510, Training Loss: 0.000495\n",
      "LBFGS Iteration 520, Training Loss: 0.000489\n",
      "LBFGS Iteration 530, Training Loss: 0.000484\n",
      "LBFGS Iteration 540, Training Loss: 0.000478\n",
      "LBFGS Iteration 550, Training Loss: 0.000473\n",
      "LBFGS Iteration 560, Training Loss: 0.000470\n",
      "LBFGS Iteration 570, Training Loss: 0.000467\n",
      "LBFGS Iteration 580, Training Loss: 0.000462\n",
      "LBFGS Iteration 590, Training Loss: 0.000459\n",
      "LBFGS Iteration 600, Training Loss: 0.000455\n",
      "LBFGS Iteration 610, Training Loss: 0.000453\n",
      "LBFGS Iteration 620, Training Loss: 0.000450\n",
      "LBFGS Iteration 630, Training Loss: 0.000446\n",
      "LBFGS Iteration 640, Training Loss: 0.000443\n",
      "LBFGS Iteration 650, Training Loss: 0.000440\n",
      "LBFGS Iteration 660, Training Loss: 0.000439\n",
      "LBFGS Iteration 670, Training Loss: 0.000438\n",
      "LBFGS Iteration 680, Training Loss: 0.000436\n",
      "LBFGS Iteration 690, Training Loss: 0.000435\n",
      "LBFGS Iteration 700, Training Loss: 0.000434\n",
      "LBFGS Iteration 710, Training Loss: 0.000432\n",
      "LBFGS Iteration 720, Training Loss: 0.000431\n",
      "LBFGS Iteration 730, Training Loss: 0.000430\n",
      "LBFGS Iteration 740, Training Loss: 0.000428\n",
      "LBFGS Iteration 750, Training Loss: 0.000427\n",
      "LBFGS Iteration 760, Training Loss: 0.000425\n",
      "LBFGS Iteration 770, Training Loss: 0.000423\n",
      "LBFGS Iteration 780, Training Loss: 0.000422\n",
      "LBFGS Iteration 790, Training Loss: 0.000419\n",
      "LBFGS Iteration 800, Training Loss: 0.000418\n",
      "LBFGS Iteration 810, Training Loss: 0.000416\n",
      "LBFGS Iteration 820, Training Loss: 0.000415\n",
      "LBFGS Iteration 830, Training Loss: 0.000413\n",
      "LBFGS Iteration 840, Training Loss: 0.000410\n",
      "LBFGS Iteration 850, Training Loss: 0.000409\n",
      "LBFGS Iteration 860, Training Loss: 0.000408\n",
      "LBFGS Iteration 870, Training Loss: 0.000406\n",
      "LBFGS Iteration 880, Training Loss: 0.000405\n",
      "LBFGS Iteration 890, Training Loss: 0.000405\n",
      "LBFGS Iteration 900, Training Loss: 0.000404\n",
      "LBFGS Iteration 910, Training Loss: 0.000402\n",
      "LBFGS Iteration 920, Training Loss: 0.000400\n",
      "LBFGS Iteration 930, Training Loss: 0.000397\n",
      "LBFGS Iteration 940, Training Loss: 0.000395\n",
      "LBFGS Iteration 950, Training Loss: 0.000393\n",
      "LBFGS Iteration 960, Training Loss: 0.000391\n",
      "LBFGS Iteration 970, Training Loss: 0.000390\n",
      "LBFGS Iteration 980, Training Loss: 0.000388\n",
      "LBFGS Iteration 990, Training Loss: 0.000387\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.LBFGS(model.parameters(), lr=1e-1, max_iter=20, max_eval=None, tolerance_grad=1e-7, tolerance_change=1e-9, history_size=100)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    out_i = model(coords_i)\n",
    "    out_b = model(coords_b)\n",
    "    loss_i = criterion(out_i, y_i)\n",
    "    loss_b = criterion(out_b, y_b)\n",
    "    loss = w_i * torch.sqrt(loss_i) + w_b * torch.sqrt(loss_b)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "# Run the LBFGS optimizer\n",
    "for i in range(1000):\n",
    "    loss = optimizer.step(closure)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"LBFGS Iteration {i}, Training Loss: {loss.item():.6f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
